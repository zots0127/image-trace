import asyncio
import json
import time
import threading
from enum import Enum
from typing import Dict, List, Tuple, Optional, Any
from uuid import UUID, uuid4

import cv2  # type: ignore[import-not-found]
import numpy as np
from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from PIL import Image as PILImage
from sqlmodel import select

from .db import get_session
from .models import AnalysisResult, AnalysisResultRead, Image
from .feature_cache import feature_cache




router = APIRouter(prefix="/analysis", tags=["analysis"])


def _compute_average_color(image_path: str) -> List[float]:
    """åŸºç¡€ç‰¹å¾ 1ï¼šå¹³å‡ RGB é¢œè‰²ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]ã€‚"""
    with PILImage.open(image_path) as img:
        img = img.convert("RGB")
        arr = np.array(img, dtype=np.float32) / 255.0
        mean_rgb = arr.reshape(-1, 3).mean(axis=0)
        return mean_rgb.tolist()


def _compute_ahash(image_path: str, size: int = 8) -> List[int]:
    """åŸºç¡€ç‰¹å¾ 2ï¼šç®€å•å¹³å‡å“ˆå¸Œ (aHash)ï¼Œè¿”å› 0/1 å‘é‡ã€‚"""
    with PILImage.open(image_path) as img:
        img = img.convert("L").resize((size, size), PILImage.Resampling.LANCZOS)
        arr = np.array(img, dtype=np.float32)
        mean = arr.mean()
        bits = (arr > mean).astype(np.uint8).flatten()
        return bits.tolist()


def _get_optimal_feature_count(image_path: str) -> int:
    """
    æ ¹æ®å›¾åƒå°ºå¯¸è‡ªåŠ¨è°ƒæ•´ORBç‰¹å¾ç‚¹æ•°é‡
    """
    try:
        with PILImage.open(image_path) as img:
            width, height = img.size
            # åŸºäºå›¾åƒé¢ç§¯è®¡ç®—ç‰¹å¾ç‚¹æ•°é‡ï¼Œæ›´åˆç†çš„åˆ†å¸ƒ
            area = width * height
            # åŸºå‡†ï¼šæ¯1000åƒç´ çº¦1ä¸ªç‰¹å¾ç‚¹
            base_features = area // 1000

            # æ ¹æ®å›¾åƒæ¯”ä¾‹è°ƒæ•´ï¼Œæ­£æ–¹å½¢å›¾åƒç‰¹å¾ç‚¹æ›´å¯†é›†
            aspect_ratio = max(width, height) / min(width, height)
            ratio_factor = 1.0 / (1.0 + (aspect_ratio - 1.0) * 0.2)  # é•¿å®½æ¯”è¶Šå¤§ï¼Œç‰¹å¾ç‚¹è¶Šç¨€ç–

            adjusted_features = int(base_features * ratio_factor)

            # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼Œæé«˜ä¸‹é™ä»¥è·å¾—æ›´å¥½çš„åŒ¹é…æ•ˆæœ
            optimal_features = min(2000, max(600, adjusted_features))
            return optimal_features
    except Exception as e:
        print(f"Error calculating optimal feature count for {image_path}: {e}")
        return 800  # å›é€€åˆ°é»˜è®¤å€¼


def _get_dynamic_distance_threshold(matches: List, percentile: float = 75) -> float:
    """
    åŸºäºåŒ¹é…è·ç¦»åˆ†å¸ƒè®¡ç®—åŠ¨æ€é˜ˆå€¼
    """
    if not matches:
        return 100  # é»˜è®¤é˜ˆå€¼

    distances = [m.distance for m in matches]
    # ä½¿ç”¨ç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼ï¼Œæ›´å…·è‡ªé€‚åº”æ€§
    threshold = float(np.percentile(distances, percentile))
    # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    return max(50, min(200, threshold))


def _filter_high_quality_matches(
    matches: List,
    image_size_factor: float = 1.0,
    min_matches: int = 10
) -> List:
    """
    å¤šå±‚é«˜è´¨é‡åŒ¹é…ç‚¹ç­›é€‰
    """
    if len(matches) < min_matches:
        return matches  # åŒ¹é…ç‚¹å¤ªå°‘ï¼Œè¿”å›å…¨éƒ¨

    # ç¬¬ä¸€å±‚ï¼šåŸºäºè·ç¦»åˆ†å¸ƒçš„åŠ¨æ€ç­›é€‰
    threshold = _get_dynamic_distance_threshold(matches, percentile=70)
    good_matches = [m for m in matches if m.distance <= threshold]

    # ç¬¬äºŒå±‚ï¼šä¿ç•™æœ€ä½³åŒ¹é…ï¼ˆåŸºäºå›¾åƒå¤§å°è°ƒæ•´æ¯”ä¾‹ï¼‰
    good_matches.sort(key=lambda m: m.distance)
    keep_ratio = max(0.2, min(0.5, 0.3 * image_size_factor))  # 20%-50%
    max_keep = max(min_matches, int(len(good_matches) * keep_ratio))
    good_matches = good_matches[:max_keep]

    # ç¬¬ä¸‰å±‚ï¼šç»Ÿè®¡ç­›é€‰ï¼Œå‰”é™¤å¼‚å¸¸å€¼
    if len(good_matches) >= 4:
        distances = [m.distance for m in good_matches]
        mean_dist = np.mean(distances)
        std_dist = np.std(distances)

        filtered_matches = []
        for m in good_matches:
            z_score = (m.distance - mean_dist) / std_dist if std_dist > 0 else 0
            if abs(z_score) < 2.0:  # ä¿ç•™2ä¸ªæ ‡å‡†å·®å†…çš„åŒ¹é…
                filtered_matches.append(m)

        # ç¡®ä¿ä¸ä¼šè¿‡åº¦ç­›é€‰
        if len(filtered_matches) >= min_matches:
            good_matches = filtered_matches

    return good_matches


def _compute_enhanced_similarity(
    matches: List,
    des1_count: int,
    des2_count: int,
    inlier_count: int = 0,
    image_size_factor: float = 1.0
) -> float:
    """
    å¢å¼ºçš„ç›¸ä¼¼åº¦è®¡ç®—ï¼Œè€ƒè™‘å¤šç»´åº¦è´¨é‡æŒ‡æ ‡
    """
    if not matches:
        return 0.0

    # åŸºç¡€åŒ¹é…æ¯”ä¾‹ï¼ˆå¯¹æè¿°å­æ•°é‡è¿›è¡Œä¸Šé™è£å‰ªï¼Œé¿å…å¤šå°ºåº¦å¯¼è‡´æ¯”ä¾‹è¿‡ä½ï¼‰
    min_descriptors = min(des1_count, des2_count)
    effective_descriptors = max(1, min(min_descriptors, 2000))
    match_ratio = len(matches) / float(effective_descriptors)

    # å†…ç‚¹è´¨é‡æƒé‡ï¼ˆå¦‚æœæœ‰RANSACç»“æœï¼‰
    inlier_ratio = 0.0
    if inlier_count > 0 and len(matches) > 0:
        inlier_ratio = inlier_count / len(matches)

    # å¹³å‡è·ç¦»è´¨é‡ï¼ˆè·ç¦»è¶Šå°ç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
    distances = [m.distance for m in matches]
    avg_distance = np.mean(distances)
    distance_score = 1.0 / (1.0 + avg_distance / 50.0)  # å½’ä¸€åŒ–åˆ°[0,1]

    # è·ç¦»ä¸€è‡´æ€§ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šå¥½ï¼‰
    distance_consistency = 1.0 / (1.0 + np.std(distances) / 20.0)

    # ç»¼åˆè¯„åˆ†æƒé‡ï¼ˆæé«˜å‡ ä½•ä¸€è‡´æ€§æƒé‡ï¼‰
    base_weights = {
        'match_ratio': 0.25,
        'inlier_ratio': 0.55 if inlier_count > 0 else 0.0,
        'distance_score': 0.15,
        'consistency': 0.05
    }

    # åŠ¨æ€è°ƒæ•´æƒé‡
    total_weight = sum(base_weights.values())
    weights = {k: v/total_weight for k, v in base_weights.items()}

    # è®¡ç®—æœ€ç»ˆåˆ†æ•°
    final_score = (
        weights['match_ratio'] * match_ratio +
        weights['inlier_ratio'] * inlier_ratio +
        weights['distance_score'] * distance_score +
        weights['consistency'] * distance_consistency
    )

    return min(1.0, final_score)


def _detect_screenshot_mode(image_path: str) -> bool:
    """
    æ£€æµ‹å›¾åƒæ˜¯å¦ä¸ºæˆªå›¾
    """
    try:
        with PILImage.open(image_path) as img:
            width, height = img.size

            # 1. æ£€æŸ¥å°ºå¯¸æ˜¯å¦ä¸ºå¸¸è§å±å¹•åˆ†è¾¨ç‡
            common_screens = [
                (1920, 1080), (1366, 768), (1536, 864), (1440, 900),
                (1280, 720), (1600, 900), (2560, 1440), (3840, 2160),
                (1080, 1920), (750, 1334), (1242, 2208), (1125, 2436)
            ]

            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘å¸¸è§å±å¹•åˆ†è¾¨ç‡ï¼ˆå…è®¸10%è¯¯å·®ï¼‰
            for screen_w, screen_h in common_screens:
                if (abs(width - screen_w) / screen_w < 0.1 and
                    abs(height - screen_h) / screen_h < 0.1):
                    return True

            # 2. æ£€æŸ¥å®½é«˜æ¯”æ˜¯å¦ä¸ºå¸¸è§å±å¹•æ¯”ä¾‹
            aspect_ratio = width / height
            common_ratios = [16/9, 16/10, 4/3, 3/2, 21/9, 9/16]
            for ratio in common_ratios:
                if abs(aspect_ratio - ratio) < 0.1:
                    return True

            # 3. æ£€æŸ¥æ–‡ä»¶å¤§å°ä¸åƒç´ æ¯”ä¾‹ï¼ˆæˆªå›¾é€šå¸¸å‹ç¼©ç‡é«˜ï¼‰
            import os
            file_size = os.path.getsize(image_path)
            pixel_count = width * height
            bytes_per_pixel = file_size / pixel_count

            # å¦‚æœæ¯åƒç´ å­—èŠ‚æ•°å¾ˆä½ï¼ˆ<0.5ï¼‰ï¼Œå¯èƒ½æ˜¯å‹ç¼©è¿‡çš„æˆªå›¾
            if bytes_per_pixel < 0.5:
                return True

            return False
    except Exception as e:
        print(f"Error detecting screenshot mode for {image_path}: {e}")
        return False


def _extract_enhanced_features(image_path: str, screenshot_mode: bool = False) -> Tuple[List, List]:
    """
    å¢å¼ºçš„ç‰¹å¾æå–ï¼Œæ”¯æŒå¤šå°ºåº¦å’Œæˆªå›¾ä¼˜åŒ–
    æ‰©å±•å°ºåº¦èŒƒå›´ä»¥æ”¯æŒå¤§æ¯”ä¾‹ç¼©æ”¾çš„å›¾åƒåŒ¹é…
    """
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return [], None

    h, w = img.shape

    # å¯¹äºéå¸¸å°çš„å›¾åƒï¼Œè·³è¿‡å¤šå°ºåº¦å¤„ç†
    min_size_for_multiscale = 100
    if h < min_size_for_multiscale or w < min_size_for_multiscale:
        print(f"Small image detected ({w}x{h}), using single-scale processing")
        scales = [1.0]  # åªä½¿ç”¨åŸå§‹å°ºåº¦
        features_per_scale = 2000  # å¢åŠ ç‰¹å¾ç‚¹æ•°é‡ä»¥è¡¥å¿
    elif screenshot_mode:
        # æˆªå›¾æ¨¡å¼ï¼šä½¿ç”¨æå¹¿èŒƒå›´çš„å°ºåº¦ä»¥å¤„ç†10å€ç”šè‡³æ›´å¤§çš„ç¼©æ”¾æ¯”ä¾‹
        # æ”¯æŒ0.1xï¼ˆ10å€ç¼©å°ï¼‰åˆ°10xï¼ˆ10å€æ”¾å¤§ï¼‰çš„ç¼©æ”¾èŒƒå›´
        # ä½¿ç”¨å¯¹æ•°åˆ†å¸ƒçš„é‡‡æ ·ç‚¹ä»¥é«˜æ•ˆè¦†ç›–å¤§èŒƒå›´
        scales = [
            0.1,   # 10å€ç¼©å°
            0.2,   # 5å€ç¼©å°
            0.35,  # çº¦3å€ç¼©å°
            0.5,   # 2å€ç¼©å°
            0.7,   # çº¦1.4å€ç¼©å°
            1.0,   # åŸå§‹å°ºåº¦
            1.4,   # çº¦1.4å€æ”¾å¤§
            2.0,   # 2å€æ”¾å¤§
            3.0,   # 3å€æ”¾å¤§
            5.0,   # 5å€æ”¾å¤§
            10.0   # 10å€æ”¾å¤§
        ]
        features_per_scale = 1500  # å¢åŠ ç‰¹å¾ç‚¹ä»¥æé«˜åŒ¹é…æˆåŠŸç‡
        print(f"ğŸ” Screenshot mode: using extreme wide-scale range (0.1x - 10x) with {len(scales)} scales")
    else:
        # å¸¸è§„æ¨¡å¼ï¼šé€‚åº¦æ‰©å±•æ ‡å‡†å¤šå°ºåº¦èŒƒå›´
        scales = [0.4, 0.6, 0.8, 1.0, 1.25, 1.6, 2.5]
        features_per_scale = 1000

    all_keypoints = []
    all_descriptors = []

    for scale in scales:
        # ç¼©æ”¾å›¾åƒï¼ˆç¡®ä¿æœ€å°å°ºå¯¸ï¼‰
        if scale != 1.0:
            h, w = img.shape
            new_h, new_w = int(h * scale), int(w * scale)

            # ç¡®ä¿ç¼©æ”¾åçš„å›¾åƒå°ºå¯¸æœ‰æ•ˆï¼ˆæœ€å°32x32ï¼‰
            new_h = max(32, new_h)
            new_w = max(32, new_w)

            # è®¡ç®—å®é™…ä½¿ç”¨çš„ç¼©æ”¾å› å­
            actual_scale_h = new_h / h
            actual_scale_w = new_w / w
            actual_scale = min(actual_scale_h, actual_scale_w)

            try:
                scaled_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
            except cv2.error as e:
                print(f"Warning: Failed to resize image with scale {scale}: {e}")
                # å¦‚æœç¼©æ”¾å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªå°ºåº¦
                continue
        else:
            scaled_img = img
            actual_scale = 1.0

        # æå–ORBç‰¹å¾
        orb = cv2.ORB_create(nfeatures=features_per_scale)
        keypoints, descriptors = orb.detectAndCompute(scaled_img, None)

        if descriptors is not None:
            # å°†å…³é”®ç‚¹åæ ‡è½¬æ¢å›åŸå§‹å›¾åƒå°ºåº¦
            scaled_keypoints = []
            for kp in keypoints:
                # ç¡®ä¿ä¸ä¼šé™¤é›¶
                if actual_scale > 0:
                    original_x = kp.pt[0] / actual_scale
                    original_y = kp.pt[1] / actual_scale
                    original_size = kp.size / actual_scale
                else:
                    # å¦‚æœç¼©æ”¾å› å­å¼‚å¸¸ï¼Œä½¿ç”¨åŸå§‹åæ ‡
                    original_x = kp.pt[0]
                    original_y = kp.pt[1]
                    original_size = kp.size

                scaled_keypoints.append(cv2.KeyPoint(
                    x=original_x, y=original_y,
                    size=original_size,
                    angle=kp.angle,
                    response=kp.response,
                    octave=kp.octave,
                    class_id=kp.class_id
                ))

            all_keypoints.extend(scaled_keypoints)
            all_descriptors.extend(descriptors)

    # åˆå¹¶æ‰€æœ‰æè¿°å­
    if all_descriptors:
        combined_descriptors = np.vstack(all_descriptors)
    else:
        combined_descriptors = None

    return all_keypoints, combined_descriptors


def _adaptive_screenshot_match_filter(matches: List, screenshot_mode: bool = False) -> List:
    """
    é’ˆå¯¹æˆªå›¾çš„è‡ªé€‚åº”åŒ¹é…è¿‡æ»¤
    æ”¯æŒæå¤§æ¯”ä¾‹å˜åŒ–ï¼ˆ10å€æˆ–æ›´å¤§ï¼‰
    """
    if not matches:
        return []

    if screenshot_mode:
        # æˆªå›¾æ¨¡å¼ï¼šéå¸¸å®½æ¾çš„åŒ¹é…æ¡ä»¶ä»¥å¤„ç†æå¤§å°ºåº¦å˜åŒ–
        # ä½¿ç”¨éå¸¸å®½æ¾çš„ç™¾åˆ†ä½æ•°é˜ˆå€¼ï¼ˆ90%ï¼‰
        threshold = _get_dynamic_distance_threshold(matches, percentile=90)

        # è¿›ä¸€æ­¥é™ä½æœ€å°åŒ¹é…è¦æ±‚
        min_matches = 3

        # ä¿ç•™æ›´å¤šåŒ¹é…ç‚¹
        good_matches = [m for m in matches if m.distance <= threshold]
        good_matches.sort(key=lambda m: m.distance)

        # å¯¹äºæå¤§æ¯”ä¾‹çš„æˆªå›¾ï¼Œä¿ç•™æ›´é«˜æ¯”ä¾‹çš„åŒ¹é…ç‚¹ï¼ˆ70%ï¼‰
        keep_count = max(min_matches, int(len(good_matches) * 0.7))
        good_matches = good_matches[:keep_count]

        # å¦‚æœè¿˜æ˜¯å¤ªå°‘ï¼Œè¿›ä¸€æ­¥æ”¾å®½æ¡ä»¶
        if len(good_matches) < min_matches and len(matches) >= min_matches:
            good_matches = sorted(matches, key=lambda m: m.distance)[:min_matches]

        return good_matches
    else:
        # å¸¸è§„æ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰çš„é«˜è´¨é‡ç­›é€‰
        return _filter_high_quality_matches(matches)


async def _compute_fast_features_cached(image_id: str, image_path: str) -> Tuple[List[float], List[int]]:
    """
    è®¡ç®—å¿«é€Ÿç‰¹å¾ï¼ˆå¹³å‡é¢œè‰² + aHashï¼‰ï¼Œä¼˜å…ˆä»ç¼“å­˜è·å–
    """
    # å°è¯•ä»ç¼“å­˜è·å–fastç‰¹å¾
    cached_fast = await feature_cache.get_image_features(image_id, "fast")
    if cached_fast:
        print(f"Fast features loaded from cache for image {image_id}")
        avg_color = cached_fast.get("avg_color_features", [])
        ahash = cached_fast.get("ahash_features", [])
        return avg_color, ahash

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè®¡ç®—ç‰¹å¾
    print(f"Computing fast features for image {image_id}")
    avg_color = _compute_average_color(image_path)
    ahash = _compute_ahash(image_path)

    # ç¼“å­˜è®¡ç®—ç»“æœ
    fast_data = {
        "avg_color_features": avg_color,
        "ahash_features": ahash,
        "computed_at": time.time()
    }

    try:
        await feature_cache.cache_image_features(image_id, {"fast": fast_data})
    except Exception as e:
        try:
            print(f"Fast feature cache skipped for {image_id}: {e}")
        except Exception:
            pass

    return avg_color, ahash


async def _batch_compute_fast_features(image_ids: List[str], image_paths: List[str]) -> Tuple[List[List[float]], List[List[int]]]:
    """
    æ‰¹é‡è®¡ç®—å¿«é€Ÿç‰¹å¾ï¼Œåˆ©ç”¨ç¼“å­˜ä¼˜åŒ–æ€§èƒ½
    """
    # æ‰¹é‡å°è¯•ä»ç¼“å­˜è·å–ç‰¹å¾
    cached_features = await feature_cache.batch_get_features(image_ids)

    avg_colors = []
    ahashes = []
    compute_tasks = []
    compute_indices = []

    # å¤„ç†å·²ç¼“å­˜çš„ç‰¹å¾
    for i, image_id in enumerate(image_ids):
        if image_id in cached_features and "fast" in cached_features[image_id].get("features", {}):
            fast_features = cached_features[image_id]["features"]["fast"]
            avg_colors.append(fast_features.get("avg_color_features", []))
            ahashes.append(fast_features.get("ahash_features", []))
        else:
            # éœ€è¦è®¡ç®—çš„ç‰¹å¾
            compute_indices.append(i)
            compute_tasks.append(_compute_fast_features_cached(image_id, image_paths[i]))

    # æ‰¹é‡è®¡ç®—ç¼ºå¤±çš„ç‰¹å¾
    if compute_tasks:
        computed_results = await asyncio.gather(*compute_tasks)

        # å°†è®¡ç®—ç»“æœæ’å…¥åˆ°æ­£ç¡®ä½ç½®
        for idx, (avg_color, ahash) in zip(compute_indices, computed_results):
            # æ‰©å±•åˆ—è¡¨ä»¥å®¹çº³æ–°ç»“æœ
            while len(avg_colors) <= idx:
                avg_colors.append([])
                ahashes.append([])

            avg_colors[idx] = avg_color
            ahashes[idx] = ahash

    # ç¡®ä¿åˆ—è¡¨é•¿åº¦æ­£ç¡®
    while len(avg_colors) < len(image_ids):
        avg_colors.append([])
        ahashes.append([])

    return avg_colors, ahashes


def _cosine_similarity_matrix(vectors: List[List[float]]) -> List[List[float]]:
    if not vectors:
        return []
    X = np.array(vectors, dtype=np.float32)
    norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-8
    X_norm = X / norms
    sim = X_norm @ X_norm.T
    return sim.astype(float).tolist()


def _ahash_similarity_matrix(hashes: List[List[int]]) -> List[List[float]]:
    if not hashes:
        return []
    H = np.array(hashes, dtype=np.uint8)
    n = H.shape[0]
    sim = np.zeros((n, n), dtype=np.float32)
    bit_len = H.shape[1]
    for i in range(n):
        sim[i, i] = 1.0
        for j in range(i + 1, n):
            dist = np.count_nonzero(H[i] ^ H[j])
            score = 1.0 - dist / float(bit_len)
            sim[i, j] = sim[j, i] = score
    return sim.astype(float).tolist()


def _orb_pairwise_analysis(
    image_paths: List[str],
) -> Tuple[List[List[float]], List[List[int]], List[Dict[str, object]]]:
    """ä½¿ç”¨ ORB å±€éƒ¨ç‰¹å¾åšä¸¤ä¸¤åŒ¹é…ï¼Œå¹¶ä¼°è®¡å±€éƒ¨åŒºåŸŸä½ç½®ã€‚
    ç°åœ¨æ”¯æŒæˆªå›¾æ£€æµ‹å’Œå¢å¼ºç‰¹å¾æå–ã€‚

    è¿”å›ï¼š
    - sim: ORB ç›¸ä¼¼åº¦çŸ©é˜µ
    - match_counts: åŒ¹é…æ•°é‡çŸ©é˜µ
    - pairwise_regions: åˆ—è¡¨ï¼Œæ¯é¡¹æè¿° source_index åœ¨ target_index ä¸­çš„ä¸€ä¸ªå€™é€‰åŒºåŸŸ
        ç°åœ¨åŒ…å«ç‰¹å¾ç‚¹è¿çº¿å¯è§†åŒ–æ•°æ®
    """
    if not image_paths:
        return [], [], []

    # æ£€æµ‹æ‰€æœ‰å›¾åƒçš„æˆªå›¾æ¨¡å¼
    screenshot_modes = []
    image_sizes = []  # è®°å½•å›¾åƒå°ºå¯¸ç”¨äºæ¯”è¾ƒ
    
    for path in image_paths:
        is_screenshot = _detect_screenshot_mode(path)
        screenshot_modes.append(is_screenshot)
        
        # è®°å½•å›¾åƒå°ºå¯¸
        try:
            with PILImage.open(path) as img:
                image_sizes.append(img.size)  # (width, height)
        except:
            image_sizes.append((0, 0))
        
        if is_screenshot:
            print(f"Detected screenshot mode for image: {path}")

    # æ£€æµ‹æ˜¯å¦æœ‰æ˜¾è‘—çš„åˆ†è¾¨ç‡å·®å¼‚ï¼ˆå¯èƒ½æ˜¯ç¼©æ”¾æˆªå›¾ï¼‰
    # æ”¯æŒæå¤§çš„æ¯”ä¾‹å·®å¼‚ï¼ˆæœ€é«˜100å€ï¼‰
    has_resolution_mismatch = False
    if len(image_sizes) >= 2:
        for i in range(len(image_sizes)):
            for j in range(i + 1, len(image_sizes)):
                w1, h1 = image_sizes[i]
                w2, h2 = image_sizes[j]
                if w1 > 0 and w2 > 0 and h1 > 0 and h2 > 0:
                    # è®¡ç®—é¢ç§¯æ¯”ä¾‹
                    area1 = w1 * h1
                    area2 = w2 * h2
                    area_ratio = max(area1, area2) / min(area1, area2)
                    
                    # è®¡ç®—å®½é«˜æ¯”ä¾‹ï¼ˆæ£€æµ‹å•ç»´åº¦çš„å·¨å¤§å·®å¼‚ï¼‰
                    width_ratio = max(w1, w2) / min(w1, w2)
                    height_ratio = max(h1, h2) / min(h1, h2)
                    max_dimension_ratio = max(width_ratio, height_ratio)
                    
                    # å¦‚æœé¢ç§¯å·®å¼‚è¶…è¿‡1.3å€ï¼Œæˆ–ä»»æ„ç»´åº¦å·®å¼‚è¶…è¿‡1.5å€ï¼Œå¯ç”¨æˆªå›¾æ¨¡å¼
                    # è¿™æ ·å¯ä»¥æ•è·10å€ç”šè‡³æ›´å¤§çš„ç¼©æ”¾æ¯”ä¾‹
                    if area_ratio > 1.3 or max_dimension_ratio > 1.5:
                        has_resolution_mismatch = True
                        print(f"âš ï¸ Resolution mismatch detected: {w1}x{h1} vs {w2}x{h2}")
                        print(f"   ğŸ“Š Area ratio: {area_ratio:.2f}x, Width ratio: {width_ratio:.2f}x, Height ratio: {height_ratio:.2f}x")
                        # å¯¹è¿™ä¸¤å¼ å›¾éƒ½å¯ç”¨æˆªå›¾æ¨¡å¼
                        screenshot_modes[i] = True
                        screenshot_modes[j] = True

    # å¦‚æœæœ‰ä»»ä½•æˆªå›¾æˆ–åˆ†è¾¨ç‡å·®å¼‚ï¼Œåˆ™å¯ç”¨æˆªå›¾ä¼˜åŒ–æ¨¡å¼
    has_screenshots = any(screenshot_modes) or has_resolution_mismatch
    if has_screenshots:
        print("ğŸ” Screenshot/scale detection enabled - using enhanced matching")

    images: List[np.ndarray | None] = []
    keypoints_list: List[list] = []
    descriptors: List[np.ndarray | None] = []

    # ä½¿ç”¨å¢å¼ºç‰¹å¾æå–
    for i, path in enumerate(image_paths):
        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            images.append(None)
            keypoints_list.append([])
            descriptors.append(None)
            continue

        # ä½¿ç”¨å¢å¼ºç‰¹å¾æå–ï¼ˆæ”¯æŒå¤šå°ºåº¦å’Œæˆªå›¾ä¼˜åŒ–ï¼‰
        keypoints, des = _extract_enhanced_features(path, screenshot_mode=screenshot_modes[i])

        images.append(img)
        keypoints_list.append(keypoints)
        descriptors.append(des)

        print(f"Image {i+1}: {len(keypoints)} keypoints extracted "
              f"{'(screenshot mode)' if screenshot_modes[i] else '(normal mode)'}")

    n = len(image_paths)
    sim = [[0.0 for _ in range(n)] for _ in range(n)]
    match_counts = [[0 for _ in range(n)] for _ in range(n)]
    pairwise_regions: List[Dict[str, object]] = []

    # ä½¿ç”¨KNNåŒ¹é…å™¨è€Œä¸æ˜¯crossCheckï¼Œå¯¹å°ºåº¦å˜åŒ–æ›´é²æ£’
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)

    for i in range(n):
        sim[i][i] = 1.0
        for j in range(i + 1, n):
            des1, des2 = descriptors[i], descriptors[j]
            if des1 is None or des2 is None:
                score = 0.0
                count = 0
                sim[i][j] = sim[j][i] = float(score)
                match_counts[i][j] = match_counts[j][i] = int(count)
                continue

            # ä½¿ç”¨KNNåŒ¹é…ï¼ˆk=2ï¼‰ä»¥åº”ç”¨Lowe's ratio test
            # è¿™å¯¹äºå¤„ç†ä¸åŒå°ºåº¦çš„å›¾åƒæ›´é²æ£’
            try:
                knn_matches = bf.knnMatch(des1, des2, k=2)
            except Exception as e:
                print(f"Warning: KNN matching failed for images {i}-{j}: {e}")
                score = 0.0
                count = 0
                sim[i][j] = sim[j][i] = float(score)
                match_counts[i][j] = match_counts[j][i] = int(count)
                continue

            # æ£€æŸ¥æ˜¯å¦æ¶‰åŠæˆªå›¾
            is_screenshot_pair = screenshot_modes[i] or screenshot_modes[j]
            is_crop_pair = False
            try:
                h1, w1 = images[i].shape[:2]
                h2, w2 = images[j].shape[:2]
                area1 = float(w1 * h1)
                area2 = float(w2 * h2)
                size_ratio = min(area1, area2) / max(area1, area2) if max(area1, area2) > 0 else 1.0
                ar1 = float(w1) / float(h1) if h1 > 0 else 1.0
                ar2 = float(w2) / float(h2) if h2 > 0 else 1.0
                aspect_diff = abs(ar1 - ar2)
                is_crop_pair = (size_ratio < 0.65) and (aspect_diff < 0.25)
            except Exception:
                is_crop_pair = False
            
            # åº”ç”¨Lowe's ratio testè¿‡æ»¤è‰¯å¥½åŒ¹é…
            # å¯¹äºæå¤§æ¯”ä¾‹å˜åŒ–çš„æˆªå›¾åœºæ™¯ä½¿ç”¨éå¸¸å®½æ¾çš„ratio
            # å› ä¸ºå¤§å°ºåº¦å˜åŒ–ä¼šå¯¼è‡´ç‰¹å¾æè¿°å­å·®å¼‚å¢å¤§
            if is_screenshot_pair or is_crop_pair:
                ratio_threshold = 0.90
            else:
                ratio_threshold = 0.85
            
            matches = []
            for match_pair in knn_matches:
                if len(match_pair) == 2:
                    m, n_match = match_pair
                    if m.distance < ratio_threshold * n_match.distance:
                        matches.append(m)
                elif len(match_pair) == 1:
                    # åªæœ‰ä¸€ä¸ªåŒ¹é…ï¼Œä¹Ÿæ¥å—
                    matches.append(match_pair[0])
            
            print(f"Images {i}-{j}: {len(knn_matches)} raw matches -> {len(matches)} after ratio test (ratio={ratio_threshold})")

            # è®¡ç®—å›¾åƒå¤§å°å› å­ç”¨äºåŠ¨æ€è°ƒæ•´
            img1_size = images[i].shape[0] * images[i].shape[1] if images[i] is not None else 1000*1000
            img2_size = images[j].shape[0] * images[j].shape[1] if images[j] is not None else 1000*1000
            avg_img_size = (img1_size + img2_size) / 2
            image_size_factor = avg_img_size / (1000*1000)  # ç›¸å¯¹äº1MPçš„åŸºå‡†

            # ä½¿ç”¨è‡ªé€‚åº”åŒ¹é…ç­›é€‰ï¼ˆæ”¯æŒæˆªå›¾æ¨¡å¼ï¼‰
            good_matches = _adaptive_screenshot_match_filter(
                matches,
                screenshot_mode=(is_screenshot_pair or is_crop_pair)
            )
            count = len(good_matches)

            # ä½¿ç”¨å¢å¼ºçš„ç›¸ä¼¼åº¦è®¡ç®—
            score = _compute_enhanced_similarity(
                good_matches,
                len(des1),
                len(des2),
                image_size_factor=image_size_factor
            )

            # æˆªå›¾æ¨¡å¼ä¸‹çš„é¢å¤–ç›¸ä¼¼åº¦è¡¥å¿ï¼ˆé’ˆå¯¹æå¤§æ¯”ä¾‹å˜åŒ–ï¼‰
            if is_screenshot_pair and count >= 3:
                # å¯¹äºæå¤§æ¯”ä¾‹çš„æˆªå›¾ï¼Œé™ä½åŒ¹é…ç‚¹è¦æ±‚å¹¶æä¾›æ›´å¤§çš„å¥–åŠ±
                # æ¯å¤šä¸€ä¸ªåŒ¹é…ç‚¹å¢åŠ 8%ï¼ˆæ¯”ä¹‹å‰çš„5%æ›´æ¿€è¿›ï¼‰
                bonus_factor = 1.0 + (count - 3) * 0.08
                score = min(1.0, score * bonus_factor)
                print(f"Screenshot match bonus applied: {count} matches, factor: {bonus_factor:.2f}, final score: {score:.3f}")

            sim[i][j] = sim[j][i] = float(score)
            match_counts[i][j] = match_counts[j][i] = int(count)

            # é™ä½å¯è§†åŒ–çš„åŒ¹é…ç‚¹è¦æ±‚ï¼ˆä»3é™åˆ°2ï¼‰
            if count >= 2 and images[i] is not None and images[j] is not None:
                # æ­¥éª¤1ï¼šå…ˆå°è¯•è®¡ç®—å•åº”çŸ©é˜µæ¥éªŒè¯å‡ ä½•ä¸€è‡´æ€§
                # è·å–inlier maskä»¥è¿‡æ»¤åªæ˜¾ç¤ºå‡ ä½•ä¸€è‡´çš„åŒ¹é…ç‚¹
                visualization_matches = good_matches
                inlier_mask = None
                H = None
                
                try:
                    src_pts = np.float32(
                        [keypoints_list[i][m.queryIdx].pt for m in good_matches]
                    ).reshape(-1, 1, 2)
                    dst_pts = np.float32(
                        [keypoints_list[j][m.trainIdx].pt for m in good_matches]
                    ).reshape(-1, 1, 2)

                    # å¯¹äºæˆªå›¾æ¨¡å¼ï¼Œä½¿ç”¨éå¸¸å®½æ¾çš„RANSACå‚æ•°ä»¥å¤„ç†æå¤§å°ºåº¦å˜åŒ–
                    if is_screenshot_pair:
                        ransac_threshold = 15.0
                        min_inliers = 2
                    elif is_crop_pair:
                        ransac_threshold = 12.0
                        min_inliers = 2
                    else:
                        ransac_threshold = 10.0
                        min_inliers = 3

                    H, inlier_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, ransac_threshold)
                    
                    if H is not None and inlier_mask is not None:
                        inliers = int(inlier_mask.ravel().sum())
                        
                        if inliers >= min_inliers:
                            # æ­¥éª¤2ï¼šæ ¹æ®maskè¿‡æ»¤å‡ºçœŸæ­£çš„inlier matches
                            # åªæœ‰è¿™äº›åŒ¹é…ç‚¹ä¼šè¢«å¯è§†åŒ–
                            visualization_matches = [
                                m for idx, m in enumerate(good_matches) 
                                if inlier_mask[idx][0] == 1
                            ]
                            print(f"âœ“ Geometric verification: {len(good_matches)} matches -> {len(visualization_matches)} inliers")
                        else:
                            print(f"âš  Too few inliers ({inliers}), showing all matches")
                            inlier_mask = None  # å†…ç‚¹å¤ªå°‘ï¼Œæ˜¾ç¤ºæ‰€æœ‰åŒ¹é…
                    else:
                        print(f"âš  Homography failed, showing all matches")
                        inlier_mask = None
                        
                except Exception as e:
                    print(f"âš  Geometric verification error: {e}, showing all matches")
                    inlier_mask = None
                
                # æ­¥éª¤3ï¼šç”Ÿæˆå¯è§†åŒ–æ•°æ®ï¼ˆåªåŒ…å«å‡ ä½•ä¸€è‡´çš„å†…ç‚¹ï¼‰
                src_keypoints = []
                dst_keypoints = []
                matches_data = []

                for idx, match in enumerate(visualization_matches):
                    # è·å–åŒ¹é…ç‚¹åœ¨æºå›¾åƒå’Œç›®æ ‡å›¾åƒä¸­çš„åæ ‡
                    src_kp = keypoints_list[i][match.queryIdx]
                    dst_kp = keypoints_list[j][match.trainIdx]

                    src_kp_data = {
                        "x": float(src_kp.pt[0]),
                        "y": float(src_kp.pt[1]),
                        "size": float(src_kp.size),
                        "angle": float(src_kp.angle)
                    }

                    dst_kp_data = {
                        "x": float(dst_kp.pt[0]),
                        "y": float(dst_kp.pt[1]),
                        "size": float(dst_kp.size),
                        "angle": float(dst_kp.angle)
                    }

                    src_keypoints.append(src_kp_data)
                    dst_keypoints.append(dst_kp_data)

                    # æ·»åŠ é…å¯¹çš„matchæ•°æ®ï¼ˆç”¨äºå‰ç«¯ç›´æ¥æ¸²æŸ“ï¼‰
                    matches_data.append({
                        "queryIdx": match.queryIdx,
                        "trainIdx": match.trainIdx,
                        "distance": float(match.distance),
                        "queryPoint": src_kp_data,
                        "trainPoint": dst_kp_data,
                        "is_inlier": True  # æ ‡è®°ä¸ºå†…ç‚¹
                    })

                # åˆå§‹åŒ–åŒºåŸŸæ•°æ®
                inlier_count = len(visualization_matches) if inlier_mask is not None else 0
                region = {
                    "source_index": i,
                    "target_index": j,
                    "image1_idx": i,
                    "image2_idx": j,
                    "score": float(score),
                    "similarity": float(score),
                    "match_count": int(count),
                    "inlier_count": inlier_count,  # å®é™…å†…ç‚¹æ•°é‡
                    "quad_in_target": None,
                    "bbox_in_target": None,
                    "matches": matches_data,  # åªåŒ…å«inliers
                    "keypoints1": src_keypoints,
                    "keypoints2": dst_keypoints,
                    "feature_matches": {
                        "source_keypoints": src_keypoints,
                        "target_keypoints": dst_keypoints,
                        "source_image_size": [images[i].shape[1], images[i].shape[0]],
                        "target_image_size": [images[j].shape[1], images[j].shape[0]],
                        "match_distances": [float(m.distance) for m in visualization_matches],
                        "all_inliers": inlier_mask is not None  # æ ‡è®°æ˜¯å¦é€šè¿‡å‡ ä½•éªŒè¯
                    }
                }

                # æ­¥éª¤4ï¼šå¦‚æœæœ‰æœ‰æ•ˆçš„å•åº”çŸ©é˜µï¼Œè®¡ç®—å˜æ¢åŒºåŸŸå’Œæ›´æ–°ç›¸ä¼¼åº¦
                if H is not None and inlier_mask is not None and inlier_count >= min_inliers:
                    try:
                        # ä½¿ç”¨å†…ç‚¹ä¿¡æ¯é‡æ–°è®¡ç®—æ›´å‡†ç¡®çš„ç›¸ä¼¼åº¦åˆ†æ•°
                        enhanced_score = _compute_enhanced_similarity(
                            visualization_matches,
                            len(des1),
                            len(des2),
                            inlier_count=inlier_count,
                            image_size_factor=image_size_factor
                        )

                        # æ›´æ–°ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä»…å½“å†…ç‚¹éªŒè¯é€šè¿‡æ—¶ï¼‰
                        region["score"] = float(enhanced_score)
                        region["similarity"] = float(enhanced_score)
                        sim[i][j] = sim[j][i] = float(enhanced_score)

                        # è®¡ç®—å˜æ¢åŒºåŸŸ
                        h1, w1 = images[i].shape[:2]
                        h2, w2 = images[j].shape[:2]

                        src_corners = np.float32(
                            [[0, 0], [w1, 0], [w1, h1], [0, h1]]
                        ).reshape(-1, 1, 2)
                        dst_corners = cv2.perspectiveTransform(src_corners, H).reshape(-1, 2)

                        xs = dst_corners[:, 0]
                        ys = dst_corners[:, 1]

                        x_min = float(xs.min())
                        y_min = float(ys.min())
                        x_max = float(xs.max())
                        y_max = float(ys.max())

                        # ç®€å•è£å‰ªåˆ°ç›®æ ‡å›¾åƒè¾¹ç•Œ
                        x_min_clamped = max(0.0, min(x_min, float(w2)))
                        y_min_clamped = max(0.0, min(y_min, float(h2)))
                        x_max_clamped = max(0.0, min(x_max, float(w2)))
                        y_max_clamped = max(0.0, min(y_max, float(h2)))

                        region["quad_in_target"] = dst_corners.tolist()
                        region["bbox_in_target"] = [
                            x_min_clamped,
                            y_min_clamped,
                            x_max_clamped,
                            y_max_clamped,
                        ]
                    except Exception as e:
                        print(f"Warning: Failed to compute transformation region: {e}")
                else:
                    # å‡ ä½•éªŒè¯å¤±è´¥çš„å¤„ç†
                    if is_screenshot_pair:
                        fallback_score = score * 0.7
                        region["score"] = float(fallback_score)
                        region["similarity"] = float(fallback_score)
                        print(f"Screenshot fallback: {fallback_score:.3f} (no geometric verification)")
                    elif is_crop_pair:
                        fallback_score = score * 0.8
                        region["score"] = float(fallback_score)
                        region["similarity"] = float(fallback_score)
                        print(f"Crop fallback: {fallback_score:.3f} (no geometric verification)")
                    else:
                        fallback_score = score * 0.5
                        region["score"] = float(fallback_score)
                        region["similarity"] = float(fallback_score)
                        print(f"Standard fallback: {fallback_score:.3f} (no geometric verification)")

                pairwise_regions.append(region)

    return sim, match_counts, pairwise_regions


def _run_analysis_task_wrapper(
    analysis_id: UUID,
    project_id: UUID
):
    """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œå¼‚æ­¥åˆ†æä»»åŠ¡çš„åŒ…è£…å‡½æ•°"""
    import asyncio
    import threading
    from concurrent.futures import ThreadPoolExecutor

    def run_in_new_loop():
        """åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(_run_analysis_task(analysis_id, project_id))
        finally:
            loop.close()

    # åœ¨çº¿ç¨‹ä¸­è¿è¡Œ
    thread = threading.Thread(target=run_in_new_loop)
    thread.start()
    return thread




async def _run_analysis_task(
    analysis_id: UUID,
    project_id: UUID
):
    """ç»Ÿä¸€åˆ†æä»»åŠ¡ï¼šæ™ºèƒ½åº”ç”¨æ‰€æœ‰åˆ†ææ–¹æ³•è·å¾—æœ€ä½³ç»“æœ

    ç»¼åˆå¿«é€Ÿåˆ†æå’Œç²¾ç¡®åˆ†æçš„ä¼˜åŠ¿ï¼š
    - å¿«é€Ÿç‰¹å¾åˆ†æï¼ˆå¹³å‡é¢œè‰² + æ„ŸçŸ¥å“ˆå¸Œï¼‰
    - ORBå±€éƒ¨ç‰¹å¾åŒ¹é…ï¼ˆæ”¯æŒæˆªå›¾æ£€æµ‹ï¼‰
    - æ™ºèƒ½æƒé‡æ··åˆï¼ˆæ ¹æ®ORBåŒ¹é…è´¨é‡åŠ¨æ€è°ƒæ•´ï¼‰
    """
    start_time = time.time()
    temp_dir = None

    try:
        await feature_cache._force_reconnect()
        print("ğŸ”„ Redis connection reset for new event loop")
    except Exception as e:
        print(f"âš  Redis reconnect failed: {e}")

    try:
        with get_session() as session:
            analysis = session.get(AnalysisResult, analysis_id)
            if not analysis:
                raise ValueError(f"Analysis not found: {analysis_id}")

            images = session.exec(select(Image).where(Image.project_id == project_id)).all()
            actual_images = [img for img in images if img.mime_type and img.mime_type.startswith('image/')]

            if not actual_images:
                raise ValueError("No actual image files found for project")

            # ä»MinIOä¸‹è½½å›¾ç‰‡æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
            import tempfile
            import os
            from .minio_client import storage_service

            temp_dir = tempfile.mkdtemp()
            print(f"Created temporary directory: {temp_dir}")

            image_paths = []
            image_ids = []

            for img in actual_images:
                try:
                    # ç¡®å®šå›¾ç‰‡æ¥æºå’Œå­˜å‚¨æ¡¶
                    bucket = "image-trace-uploads"  # é»˜è®¤å­˜å‚¨æ¡¶

                    # æ£€æŸ¥å›¾ç‰‡å…ƒæ•°æ®ç¡®å®šæ˜¯å¦ä¸ºæ–‡æ¡£æå–å›¾ç‰‡
                    if img.image_metadata:
                        try:
                            metadata = json.loads(img.image_metadata)
                            if metadata.get("source") == "document_extraction":
                                bucket = "image-trace-extracted"
                        except:
                            pass

                    # ä»MinIOä¸‹è½½æ–‡ä»¶
                    file_data = storage_service.download_file(
                        object_name=img.file_path,
                        bucket=bucket
                    )

                    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                    temp_path = os.path.join(temp_dir, img.filename)
                    with open(temp_path, "wb") as f:
                        f.write(file_data)

                    image_paths.append(temp_path)
                    image_ids.append(str(img.id))
                    print(f"Downloaded image: {img.filename}")

                except Exception as e:
                    print(f"Failed to download image {img.filename}: {e}")
                    # ç»§ç»­å¤„ç†å…¶ä»–å›¾åƒï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                    continue

            if not image_paths:
                raise ValueError("Failed to download any images from MinIO")

            # æœåŠ¡å¥åº·æ£€æŸ¥
            print("Performing service health checks...")

            # æ£€æŸ¥Redisè¿æ¥ï¼ˆä¸å¯ç”¨æ—¶ç»§ç»­æ‰§è¡Œï¼‰
            try:
                await feature_cache.ping()
                print("âœ… Redis connection: OK")
            except Exception as e:
                print(f"âš  Redis unavailable, proceeding without cache: {e}")

            try:
                from .minio_client import storage_service
                if getattr(storage_service, "_available", False):
                    buckets = storage_service.client.list_buckets()
                    print(f"âœ… MinIO connection: OK (found {len(buckets)} buckets)")
                else:
                    print("âœ… MinIO local filesystem fallback: OK")
            except Exception as e:
                print(f"âš  MinIO connection check failed: {e}")
                print("Proceeding with local filesystem fallback")

            # æ›´æ–°çŠ¶æ€
            analysis.status = "running"
            analysis.algorithm_type = "unified"
            session.commit()

            # é˜¶æ®µ1ï¼šå¿«é€Ÿç‰¹å¾åˆ†æ
            print("Starting unified analysis - Phase 1: Fast features (color + hash)")
            analysis.progress = 0.1
            session.commit()

            fast_avg_colors, fast_ahashes = await _batch_compute_fast_features(image_ids, image_paths)
            fast_sim = _cosine_similarity_matrix(fast_avg_colors)
            fast_ahash_sim = _ahash_similarity_matrix(fast_ahashes)

            # ç»¼åˆå¿«é€Ÿç›¸ä¼¼åº¦ï¼ˆ50%é¢œè‰² + 50%å“ˆå¸Œï¼‰
            combined_fast_sim = []
            for i in range(len(fast_sim)):
                row = []
                for j in range(len(fast_sim[i])):
                    combined_score = 0.5 * fast_sim[i][j] + 0.5 * fast_ahash_sim[i][j]
                    row.append(combined_score)
                combined_fast_sim.append(row)

            analysis.progress = 0.4
            session.commit()

            # é˜¶æ®µ2ï¼šORBå±€éƒ¨ç‰¹å¾åˆ†æï¼ˆåŒ…å«æˆªå›¾ä¼˜åŒ–ï¼‰
            print("Phase 2: ORB local features with screenshot detection")
            orb_sim, orb_match_counts, orb_regions = _orb_pairwise_analysis(image_paths)

            analysis.progress = 0.7
            session.commit()

            # é˜¶æ®µ3ï¼šæ™ºèƒ½æƒé‡æ··åˆè®¡ç®—
            print("Phase 3: Dynamic weight hybrid computation")
            final_sim = []

            for i in range(len(combined_fast_sim)):
                row = []
                for j in range(len(combined_fast_sim[i])):
                    # åŠ¨æ€æƒé‡è°ƒæ•´
                    fast_score = combined_fast_sim[i][j]
                    orb_score = orb_sim[i][j]

                    # å¦‚æœORBåŒ¹é…åº¦é«˜ï¼Œå¢åŠ å…¶æƒé‡
                    if orb_score > 0.3:
                        # é«˜è´¨é‡ORBåŒ¹é…ï¼šORBæƒé‡70%ï¼Œå¿«é€Ÿç‰¹å¾30%
                        hybrid_score = 0.7 * orb_score + 0.3 * fast_score
                    elif orb_score > 0.1:
                        # ä¸­ç­‰è´¨é‡ORBåŒ¹é…ï¼šORBæƒé‡50%ï¼Œå¿«é€Ÿç‰¹å¾50%
                        hybrid_score = 0.5 * orb_score + 0.5 * fast_score
                    else:
                        # ä½è´¨é‡ORBåŒ¹é…ï¼šå¿«é€Ÿç‰¹å¾æƒé‡70%ï¼ŒORBæƒé‡30%
                        hybrid_score = 0.3 * orb_score + 0.7 * fast_score

                    row.append(hybrid_score)
                final_sim.append(row)

            # ä¿å­˜ç»“æœ
            analysis.results = json.dumps({
                "similarity_matrix": final_sim,
                "fast_similarity": combined_fast_sim,
                "orb_similarity": orb_sim,
                "match_counts": orb_match_counts,
                "pairwise_regions": orb_regions,  # ä¿ç•™å‘åå…¼å®¹
                "orb": {
                    "pairwise_regions": orb_regions,  # å‰ç«¯æœŸæœ›çš„è·¯å¾„
                    "match_counts": orb_match_counts,
                    "similarity_matrix": orb_sim
                },
                "analysis_method": "unified",
                "strategy": "dynamic_weighting",
                "screenshot_detection": "enabled",
                "features": ["color_histogram", "perceptual_hash", "orb_local_features"]
            })

            analysis.status = "completed"
            analysis.progress = 1.0
            from datetime import datetime
            analysis.completed_at = datetime.utcnow()
            session.commit()

            print(f"Unified analysis completed in {time.time() - start_time:.2f}s")

    except Exception as e:
        # é”™è¯¯å¤„ç†
        print(f"Unified analysis failed: {e}")
        import traceback
        traceback.print_exc()

        # æ›´æ–°åˆ†æçŠ¶æ€ä¸ºå¤±è´¥
        try:
            with get_session() as session:
                analysis = session.get(AnalysisResult, analysis_id)
                if analysis:
                    analysis.status = "failed"
                    analysis.error_message = f"Unified analysis failed: {str(e)}"
                    analysis.progress = 1.0
                    session.commit()
        except Exception as db_error:
            print(f"Failed to update analysis status: {db_error}")

    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_dir and os.path.exists(temp_dir):
            try:
                import shutil
                shutil.rmtree(temp_dir, ignore_errors=True)
                print(f"Cleaned up temporary directory: {temp_dir}")
            except Exception as cleanup_error:
                print(f"Failed to cleanup temporary directory: {cleanup_error}")
        
        # å…³é—­Redisè¿æ¥ä»¥é¿å…è¿æ¥æ³„æ¼
        try:
            await feature_cache._force_reconnect()
            print("ğŸ”„ Redis connection closed for event loop cleanup")
        except Exception as redis_cleanup_error:
            print(f"Warning: Failed to cleanup Redis connection: {redis_cleanup_error}")


@router.post("/start")
def start_analysis(
    background_tasks: BackgroundTasks,
    project_id: UUID,
) -> Dict[str, str]:
    """å›¾åƒç›¸ä¼¼åº¦åˆ†æå…¥å£ï¼ˆå¼‚æ­¥ï¼‰ã€‚

    ä½¿ç”¨ç»Ÿä¸€åˆ†æç­–ç•¥ï¼Œæ™ºèƒ½åº”ç”¨æ‰€æœ‰åˆ†ææ–¹æ³•ï¼š
    - å¿«é€Ÿç‰¹å¾åˆ†æï¼ˆå¹³å‡é¢œè‰² + æ„ŸçŸ¥å“ˆå¸Œï¼‰
    - ORBå±€éƒ¨ç‰¹å¾åŒ¹é…ï¼ˆæ”¯æŒæˆªå›¾æ£€æµ‹ï¼‰
    - æ™ºèƒ½æƒé‡æ··åˆï¼ˆæ ¹æ®åŒ¹é…è´¨é‡åŠ¨æ€è°ƒæ•´æƒé‡ï¼‰

    è¿”å›task_idï¼Œå¯é€šè¿‡è½®è¯¢è·å–åˆ†æè¿›åº¦å’Œç»“æœã€‚
    """
    task_id = f"task-{uuid4()}"

    with get_session() as session:
        images = session.exec(select(Image).where(Image.project_id == project_id)).all()
        actual_images = [img for img in images if img.mime_type and img.mime_type.startswith('image/')]
        if not actual_images:
            raise HTTPException(status_code=400, detail="No actual image files found for project")

        # åˆ›å»ºåˆ†æè®°å½•
        analysis = AnalysisResult(
            project_id=project_id,
            task_id=task_id,
            algorithm_type="unified",
            parameters=json.dumps({
                "strategy": "unified",
                "methods": ["fast", "orb", "hybrid_weighting"],
                "screenshot_detection": True,
                "dynamic_weighting": True
            }),
            status="pending",
            progress=0.0
        )
        session.add(analysis)
        session.commit()
        session.refresh(analysis)

        # å¯åŠ¨ç»Ÿä¸€åˆ†æä»»åŠ¡
        background_tasks.add_task(_run_analysis_task_wrapper, analysis.id, project_id)

        return {"task_id": task_id, "analysis_id": str(analysis.id)}


@router.get("/results/{analysis_id}")
def get_results(analysis_id: UUID) -> dict:
    with get_session() as session:
        analysis = session.get(AnalysisResult, analysis_id)
        if not analysis:
            raise HTTPException(status_code=404, detail="Analysis result not found")

        # è§£æJSONå­—ç¬¦ä¸²ä¸ºå­—å…¸
        result_dict = {
            "analysis_id": str(analysis.id),
            "project_id": str(analysis.project_id),
            "task_id": analysis.task_id,
            "algorithm_type": analysis.algorithm_type,
            "parameters": json.loads(analysis.parameters) if analysis.parameters else None,
            "results": json.loads(analysis.results) if analysis.results else None,
            "confidence_score": analysis.confidence_score,
            "processing_time_seconds": analysis.processing_time_seconds,
            "status": analysis.status,
            "progress": analysis.progress,
            "error_message": analysis.error_message,
            "created_at": analysis.created_at.isoformat(),
        }
        return result_dict


@router.get("/status/{analysis_id}")
def get_status(analysis_id: UUID):
    with get_session() as session:
        analysis = session.get(AnalysisResult, analysis_id)
        if not analysis:
            raise HTTPException(status_code=404, detail="Analysis result not found")

        return {
            "analysis_id": str(analysis.id),
            "task_id": analysis.task_id,
            "status": analysis.status,
            "progress": analysis.progress,
            "error_message": analysis.error_message
        }


@router.get("/cache/stats")
async def get_cache_stats():
    """è·å–ç‰¹å¾ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = await feature_cache.get_cache_stats()
        return {
            "success": True,
            "stats": stats
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get cache stats: {str(e)}")


@router.delete("/cache/image/{image_id}")
async def invalidate_image_cache(image_id: UUID):
    """ä½¿æŒ‡å®šå›¾åƒçš„ç‰¹å¾ç¼“å­˜å¤±æ•ˆ"""
    try:
        success = await feature_cache.invalidate_image_cache(str(image_id))
        return {
            "success": success,
            "message": f"Cache for image {image_id} {'invalidated' if success else 'failed to invalidate'}"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to invalidate cache: {str(e)}")


@router.post("/cache/cleanup")
async def cleanup_expired_cache():
    """æ¸…ç†è¿‡æœŸçš„ç‰¹å¾ç¼“å­˜"""
    try:
        cleaned_count = await feature_cache.cleanup_expired_features()
        return {
            "success": True,
            "cleaned_keys": cleaned_count,
            "message": f"Cleaned up {cleaned_count} expired cache keys"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to cleanup cache: {str(e)}")


@router.get("/cache/health")
async def cache_health_check():
    """Redisç¼“å­˜å¥åº·æ£€æŸ¥"""
    try:
        # æµ‹è¯•Redisè¿æ¥
        client = await feature_cache.async_redis_client
        await client.ping()

        # è·å–åŸºæœ¬ç»Ÿè®¡
        info = await client.info()

        return {
            "status": "healthy",
            "redis_connected": True,
            "redis_url": feature_cache.redis_url,
            "used_memory": info.get("used_memory_human", "N/A"),
            "connected_clients": info.get("connected_clients", "N/A"),
            "uptime_seconds": info.get("uptime_in_seconds", "N/A")
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "redis_connected": False,
            "redis_url": feature_cache.redis_url,
            "error": str(e)
        }
